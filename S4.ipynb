{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regular expressions:\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For web scraping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn package for unsupervised learning:\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For further cleaning\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nltk` package for textual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and download popular text corpa\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentiment analysis:\n",
    "import nltk.sentiment\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text cleaning:\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual analysis with some Friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file [Friends_Transcript.txt](https://raw.githubusercontent.com/ahmedgc/2024_insead_python_workshop/main/Friends_Transcript.txt) contains text data: the script for the pilot episode of the TV series \"Friends\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://m.media-amazon.com/images/M/MV5BNDVkYjU0MzctMWRmZi00NTkxLTgwZWEtOWVhYjZlYjllYmU4XkEyXkFqcGdeQXVyNTA4NzY1MzY@._V1_FMjpg_UX1000_.jpg\" \n",
    "alt=\"Friends\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from the text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by inspecting the first 10 lines of this text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_number = 1\n",
    "with open(\"Friends_Transcript.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line_number <= 10:\n",
    "            print(line)\n",
    "        line_number = line_number + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in all the lines in the file, and gather them in a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = []\n",
    "with open(\"Friends_Transcript.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        all_lines = all_lines + [line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the show, there are 6 recurring characters (Chandler, Joey, Monica, Phoebe, Rachel, Ross). Let's extract only the lines that pertain to dialog spoken by one of these main characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_lines = [line for line in all_lines if (\n",
    "    line.startswith('Chandler:') | line.startswith('Joey:') | line.startswith('Monica:') \n",
    "    | line.startswith('Phoebe:') | line.startswith('Rachel:') | line.startswith('Ross:')\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main_dialog_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_lines[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can see that there is still text that is situational, and not part of a dialog: it's the text in between parentheses () and square brackets []. So we need to eliminate it. \n",
    "\n",
    "This uses a **regular expression** to detect any text that is in between parenthese () and replace it with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_lines = [re.sub(\"\\(.*?\\)\", \"\", line) for line in main_dialog_lines]\n",
    "main_dialog_lines[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses another regular expression to detect any text that is in between square brackets [] and replace it with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_lines = [re.sub(\"\\[.*?\\]\", \"\", line) for line in main_dialog_lines]\n",
    "main_dialog_lines[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate out the subject doing the talking from what they say. This will help us identify speech later.\n",
    "\n",
    "So instead of a list of strings, where each string is a line, we want a list of *tuples*, split into the person talking and the words being said.\n",
    "\n",
    "Here are some examples of what we intend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Chandler: Happy birthday, pal! \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Chandler: Happy birthday, pal! \\n'.split(':', maxsplit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple('Chandler: Happy birthday, pal! \\n'.split(':', maxsplit=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_tupes = [tuple(line.split(':', maxsplit=1)) for line in main_dialog_lines]\n",
    "main_dialog_tupes[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate leading whitespace in the second element of each tupe\n",
    "main_dialog_tupes = [(subject, speech.strip()) for (subject, speech) in main_dialog_tupes]\n",
    "main_dialog_tupes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: \"crazy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data above (`main_dialog_lines` or `main_dialog_tupes`), count how many times each of the main characters says the word \"crazy\" (or \"Crazy\").\n",
    "\n",
    "*Hint*: you may want to use the function `.count()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_tupes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solution using main_dialog_tupes\n",
    "all_counts = [(subject, line.lower().count('crazy')) for (subject, line) in main_dialog_tupes]\n",
    "\n",
    "def extract_count(subject_name):\n",
    "    subject_counts = [count for (subject, count) in all_counts if subject == subject_name]\n",
    "    return sum(subject_counts)\n",
    "\n",
    "print('Chandler says it ' + str(extract_count('Chandler')) + ' times')\n",
    "print('Joey says it '     + str(extract_count('Joey'))     + ' times')\n",
    "print('Monica says it '   + str(extract_count('Monica'))   + ' times')\n",
    "print('Phoebe says it '   + str(extract_count('Phoebe'))   + ' times')\n",
    "print('Rachel says it '   + str(extract_count('Rachel'))   + ' times')\n",
    "print('Ross says it '     + str(extract_count('Ross'))     + ' times')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas Data Frames with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes are always useful, so let's convert from a list of tupes to a dataframe of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df = pd.DataFrame(main_dialog_tupes, columns =['Subject', 'Speech'])\n",
    "main_dialog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['Speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['Speech'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['Speech'].str.lower().str.count('crazy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['crazy_counts'] = main_dialog_df['Speech'].str.lower().str.count('crazy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df.groupby('Subject')['crazy_counts'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz** Why does the next cell give a different answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['crazy_mentioned'] = main_dialog_df['Speech'].str.lower().str.contains('crazy')\n",
    "main_dialog_df.groupby('Subject')['crazy_mentioned'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER is one methodology developed in an [academic paper](https://ojs.aaai.org/index.php/ICWSM/article/view/14550) to measure the sentiment of text. There's an implementation of this tool in the `nltk` Python package, which we will use now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an object that will perform sentiment analysis for us\n",
    "analyzer = nltk.sentiment.vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores('I hate Mondays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores('I love Fridays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores('Learning Python is interesting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.polarity_scores('Learning Python is interesting')['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['Speech'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['vader_sentiment'] = [analyzer.polarity_scores(speech)['compound'] for speech in main_dialog_df['Speech'].values] \n",
    "main_dialog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df.groupby('Subject')['vader_sentiment'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df.hist(column='vader_sentiment', by='Subject', sharex=True, sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many words are used in each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['Speech'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df['word_count'] = [len(words) for words in main_dialog_df['Speech'].str.split()]\n",
    "main_dialog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df.groupby('Subject')['word_count'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df.groupby('Subject')['word_count'].max().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df.hist(column='word_count', by='Subject', sharex=True, sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which characters get the most airtime in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df.groupby('Subject')['Speech'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dialog_df.groupby('Subject')['word_count'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning at the United Nations General Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and very basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_nations_df = pd.read_csv('un-general-debates-blueprint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_nations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_nations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_nations_df.loc[0,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_nations_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all numbers using a regular expression\n",
    "united_nations_df['text'] = united_nations_df['text'].str.replace('\\d', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_nations_df.loc[0,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As part of the processing, it's common to exclude common words that are uninformative (\"stop words\").\n",
    "# We'll use a predefined set in the nltk package. \n",
    "# We'll pass this to the next step.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF on word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `scikit-learn` [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) object to split each speech into words (tokens), remove stop-words, set words to lowercase, and extract the count of each word that appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_text_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(united_nations_df['text'])\n",
    "count_text_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_text_vectors[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_text_vectorizer.get_feature_names_out()[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an NMF model with 5 topics to vectors of word counts\n",
    "nmf_text_model = NMF(n_components=5, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(count_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to display the most important words in each topic:\n",
    "def display_topics(model, features, no_top_words=10):\n",
    "    for topic, word_vector in enumerate(model.components_):\n",
    "        total = word_vector.sum()\n",
    "        largest = word_vector.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\" %s (%2.2f)\" % (features[largest[i]],\n",
    "            word_vector[largest[i]]*100.0/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting topics from the fitted model based on the most important words in each:\n",
    "display_topics(nmf_text_model, count_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above topics don't look very distinct to my eyes, so this didn't do a great job. We can:\n",
    "* Improve our processing\n",
    "* Try a different number of topics\n",
    "* Try a different machine learning model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work on our processing. Instead of using every speech, let's use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) object to do some extra processing steps:\n",
    "* Ignore very rare words that appear less than 5 times in all speeches: these introduce noise.\n",
    "* Ignore very common words that appear in more than 70% of all speeches: these also introduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_text_vectorizer = CountVectorizer(stop_words=stop_words, min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(united_nations_df['text'])\n",
    "count_text_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an NMF model with 5 topics to vectors of word counts\n",
    "nmf_text_model = NMF(n_components=5, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(count_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics from the fitted model:\n",
    "display_topics(nmf_text_model, count_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above topics look more interpretable to me.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF on TF-IDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out another form of pre-processing. Instead of simple word counts, which are also called term frequencies $TF$, let's use TF-IDF scores.\n",
    "\n",
    "A Term Frequency-Inverse Document Frequency (TF-IDF) score is another way (besides word count) of measuring the importance of a term $t$ in a document $d$:\n",
    "$$ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) * \\text{IDF}(t) $$\n",
    "\n",
    "The `scikit-learn` [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) object computes the IDF as follows:\n",
    "\n",
    "`idf(t) = log [ n / df(t) ] + 1`, where `n` is the total number of documents.\n",
    "\n",
    "As you can see, this score normalizes the term frequency by how common or rare it is across all documents. The IDF is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors of TF-IDF scores\n",
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(united_nations_df['text'])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_text_vectors[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an NMF model with 5 topics to vectors of TF-IDF scores\n",
    "nmf_text_model = NMF(n_components=5, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics from the fitted model:\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the NMF model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a conceptual illustration of how this works. We began by creating a Document-Term Matrix (DTM), which counts how many times certain words appear in each document. Under the hood, this is a matrix. And matrices can be factorized into products of other matrices, under certain conditions.\n",
    "\n",
    "NMF stands for Non-Negative Matrix Factorization. The unsupervised machine learning model we used above performs NMF on the DTM to extract a list of topics:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Cdk8UXkHqkLxfPEFTNEU4A.jpeg\" width=\"75%\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to estimate the topics in text, with their own strengths & weaknesses. The NMF model:\n",
    "* is relatively simple\n",
    "* is very fast\n",
    "* can perform well: an NMF algorithm won the famous [Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having said that, probabilistic methods seem to be favored nowadays, especially Latent Dirichlet Allocation (LDA). However, these are too slow to demonstrate in the workshop, so you'll do this as an exercise yourself (later). I will shortly show you another model (SVD), but for now let's explore the NMF results further...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the weights/loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_text_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df = pd.DataFrame(W_text_matrix)\n",
    "weights_df.columns = [('weight_on_topic_' + str(c)) for c in weights_df.columns.values]\n",
    "weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_nations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_nations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating DataFrames side-by-side/horizontally can be dangeous, but we know that they refer to the same subjects...\n",
    "year_country_weights_df = pd.concat([\n",
    "      united_nations_df[['year', 'country_name']]\n",
    "    , weights_df\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_country_weights_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_country_weights_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each year + country, there is 1 speech only\n",
    "year_country_weights_df.groupby(['year', 'country_name']).value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize at a country level\n",
    "year_country_weights_df.groupby('country_name').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running this line from above to remind us what the fitted model topics are:\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_country_weights_df.drop(columns='year').groupby('country_name').mean().sort_values('weight_on_topic_0', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_country_weights_df.drop(columns='year').groupby('country_name').mean().sort_values('weight_on_topic_1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_country_weights_df.drop(columns='year').groupby('country_name').mean().sort_values('weight_on_topic_2', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_country_weights_df.drop(columns='year').groupby('country_name').mean().sort_values('weight_on_topic_3', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_country_weights_df.drop(columns='year').groupby('country_name').mean().sort_values('weight_on_topic_4', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_country_weights_df.drop(columns=['year', 'country_name']).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How shall we interpret the high negative correlations between weights on topic_0 and topic_1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an NMF model with 10 topics to vectors of TF-IDF scores\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics from the fitted model:\n",
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD on TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_text_model = TruncatedSVD(n_components = 5, random_state=42)\n",
    "W_svd_text_matrix = svd_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_svd_text_matrix = svd_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(svd_text_model, tfidf_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the negative values: this means that the words reject the topic. \n",
    "\n",
    "In this case, it's harder to interpret the topics because there are so many of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on word counts - try this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA (Latent Dirichlet Allocation) is a popular probabilistic model, and estimating it requires some simulation. Therefore it takes a longer time to estimate. \n",
    "\n",
    "I'll leave it for you to **try out later**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running LDA using the scikit-learn package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code, commented-out. Simply uncomment and run yourself after class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_text_model = LatentDirichletAllocation(n_components = 5, random_state=42)\n",
    "#W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "#H_lda_text_matrix = lda_text_model.components_\n",
    "    \n",
    "#display_topics(lda_text_model, count_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running LDA using the gensim package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz for later**: a popular implementation of LDA is in the `gensim` package. As an exercise, import that package yourself, and use it to fit an LDA model to the same United Nations speeches dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a recap of the cleaning we have applied in our analysis above:\n",
    "* Removing digits\n",
    "* Splitting into words\n",
    "* Lowercasing all words\n",
    "* Filtering out stop-words\n",
    "* Filtering out very rare terms\n",
    "* Filtering out very common terms\n",
    "\n",
    "And we also saw a preprocessing step of scaling to TF-IDF scores.\n",
    "\n",
    "There may be other forms of cleaning and proessing that are appropriate for your specific example.\n",
    "\n",
    "Two forms of cleaning that are popular are \"stemming\" and \"lemmatization\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload (dirty) United Nations speech data\n",
    "united_nations_df = pd.read_csv('un-general-debates-blueprint.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = united_nations_df.loc[0,'text']\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some standard steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lowercase\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove line breaks, replacing with spaces\n",
    "text = re.sub(r'\\n', ' ', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "text = text.translate(translator)\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into a list of words at runs of whitespace\n",
    "text = text.split()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "# You could add more:\n",
    "#stop_words = stop_words + ['hi', 'im']\n",
    "text = [word for word in text if not word in stop_words]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sophisticated removal of digits\n",
    "text = [re.sub(r'\\w*\\d\\w*', '', w) for w in text]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude any empty strings remaining\n",
    "text = [w for w in text if w != '']\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmers remove morphological affixes from words, leaving only the word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer object to be used below\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemmer.stem(\"things\"))\n",
    "print(stemmer.stem(\"geese\"))\n",
    "\n",
    "print(stemmer.stem(\"running\"))\n",
    "print(stemmer.stem(\"ran\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stemmed = [stemmer.stem(w) for w in text]\n",
    "text_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of lemmatization is to convert a word to its dictionary form (\"lemma\"), which may involve analyzing the entire sentence with more sophisticated techniques in order to determine the part-of-speech. We'll do something simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer object to be used below\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"things\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"running\"))\n",
    "print(lemmatizer.lemmatize(\"ran\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lemmatized = [lemmatizer.lemmatize(w) for w in text]\n",
    "text_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-scraping BoxOfficeMojo, with a little help from ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top-grossing movies of all time? \n",
    "\n",
    "BoxOfficeMojo calculated the numbers, and displays them here: https://www.boxofficemojo.com/chart/top_lifetime_gross/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT is often very useful for web-scraping. I asked it to write the first draft of code to scrape the above page:\n",
    "\n",
    "> Write Python code to extract the tables from this web page: https://www.boxofficemojo.com/chart/top_lifetime_gross/\n",
    "\n",
    "Here's what ChatGPT replied to me:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page\n",
    "url = \"https://www.boxofficemojo.com/chart/top_lifetime_gross/\"\n",
    "\n",
    "# Sending a request to fetch the content of the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Finding the table\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extracting the table headers\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Extracting the rows\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "    cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "    if cells:  # Avoid empty rows\n",
    "        rows.append(cells)\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suggested code did a fantastic job! But what's going on? \n",
    "\n",
    "Load up https://www.boxofficemojo.com/chart/top_lifetime_gross/ in your favorite browser (Chrome, Safari, ...) Then right-click on the page and View Source. \n",
    "\n",
    "Alternatively, if you are running Chrome like I am, copy-paste this into your address bar:\n",
    "\n",
    "`view-source:https://www.boxofficemojo.com/chart/top_lifetime_gross/`\n",
    "\n",
    "We'll look at it together and review the HTML. It should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `soup` contains a parsed HTML document. HTML is a hierarchical way of storing data on (every) website, including BoxOfficeMojo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python variable called `table` contains the contents of the one and only HTML `<table>` element on the page. (It's possible to have more than one table per page, so you'd need to extract the one you want).\n",
    "\n",
    "The hierarchy looks something like this:\n",
    "\n",
    "* `<table>`\n",
    "  * `<tr>` (for table rows)\n",
    "    * `<td>` for cells or `<td>` for header cells\n",
    "        * Content of the cells, which itself be yet another HTML tag\n",
    "   \n",
    "And there are other HTML elements in there, such as `<a>` for links, which have an `href` \"attribute\". Other elements can have an \"id\" attribute, which might be useful for web-parsing too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap this up into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_boxoffice_mojo_table_text(url):\n",
    "    # Sending a request to fetch the content of the page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Finding the table\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    # Extracting the table headers\n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "    \n",
    "    # Extracting the rows\n",
    "    rows = []\n",
    "    for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "        cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "        if cells:  # Avoid empty rows\n",
    "            rows.append(cells)\n",
    "    \n",
    "    # Creating a DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_boxoffice_mojo_table_text(\"https://www.boxofficemojo.com/chart/top_lifetime_gross/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That page only shows the first 200 rows. \n",
    "\n",
    "The next 200 are displayed on another page:\n",
    "https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=200\n",
    "\n",
    "So we can run our function there: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_boxoffice_mojo_table_text(\"https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a pattern:\n",
    "\n",
    "https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=0\n",
    "\n",
    "https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=200\n",
    "\n",
    "https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=400\n",
    "\n",
    "https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=600\n",
    "\n",
    "https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put these together in a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df = pd.concat([\n",
    "    scrape_boxoffice_mojo_table_text(\"https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=\" + str(offset)) \n",
    "    for offset in [0, 200, 400, 600, 800]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(domestic_lifetime_gross_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ranks are since the release date, but we should really normalize by the number of years and see the gross per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df['Year'] = domestic_lifetime_gross_df['Year'].astype(int)\n",
    "domestic_lifetime_gross_df['Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df['Lifetime Gross'] = domestic_lifetime_gross_df['Lifetime Gross'].str.replace('$','')\n",
    "domestic_lifetime_gross_df['Lifetime Gross'] = domestic_lifetime_gross_df['Lifetime Gross'].str.replace(',','')\n",
    "domestic_lifetime_gross_df['Lifetime Gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df['Lifetime Gross'] = pd.to_numeric(domestic_lifetime_gross_df['Lifetime Gross'])\n",
    "domestic_lifetime_gross_df['Lifetime Gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df['years_since_release'] = 2025 - domestic_lifetime_gross_df['Year']\n",
    "domestic_lifetime_gross_df['years_since_release']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df.plot(x='years_since_release', y='Lifetime Gross', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df['annualized_lifetime_gross'] = domestic_lifetime_gross_df['Lifetime Gross'] / domestic_lifetime_gross_df['years_since_release']\n",
    "domestic_lifetime_gross_df['annualized_lifetime_gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_lifetime_gross_df.sort_values('annualized_lifetime_gross', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an important modification to the code that ChatGPT suggested to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page\n",
    "url = \"https://www.boxofficemojo.com/chart/top_lifetime_gross/\"\n",
    "\n",
    "# Sending a request to fetch the content of the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Finding the table\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extracting the table headers\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Extracting the rows\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "    cells = []\n",
    "    for cell in row.find_all('td'):\n",
    "\n",
    "        # MODIFIED:\n",
    "        if cell.find('a'):\n",
    "            # Keeping the <a> tag as a string\n",
    "            link = cell.find('a')\n",
    "            cells.append(str(link))\n",
    "        else:\n",
    "            cells.append(cell.text.strip())\n",
    "            \n",
    "    if cells:  # Avoid empty rows\n",
    "        rows.append(cells)\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the code checks inside table cells (`<td>...</td>`) and if the cell contains an HTML link (`<a>`) it returns the actual HTML tag instead of the innermost text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page\n",
    "url = \"https://www.boxofficemojo.com/chart/top_lifetime_gross/\"\n",
    "\n",
    "# Sending a request to fetch the content of the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Finding the table\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extracting the table headers\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Extracting the rows\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "    cells = []\n",
    "    for cell in row.find_all('td'):\n",
    "        link = cell.find('a')\n",
    "        if link:\n",
    "            \n",
    "            # MODIFIED:\n",
    "            # Extracting only the href attribute\n",
    "            cells.append(link.get('href'))\n",
    "            \n",
    "        else:\n",
    "            cells.append(cell.text.strip())\n",
    "    if cells:  # Avoid empty rows\n",
    "        rows.append(cells)\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have now extracted the contents of the `href` attribute within the `<a>` link tags, where they exist. Otherwise, the text is extracted.\n",
    "\n",
    "The title identifiers seem to be valuable. If you click on \"Star Wars: Episode VII - The Force Awakens\" in the first row, it takes you to this page:\n",
    "\n",
    "https://www.boxofficemojo.com/title/tt2488496/?ref_=bo_cso_table_1\n",
    "\n",
    "And you can see that we have this data in the first row of our scraped table now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1, 'Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use this to construct a URL ourselves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1, 'Title'].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1, 'Title'].split('/')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.boxofficemojo.com/title/' + df.loc[1, 'Title'].split('/')[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to change the code to run on the entire column, rather than a single cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'].str.split('/').str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://www.boxofficemojo.com/title/' + df['Title'].str.split('/').str[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine this with the previous code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page\n",
    "url = \"https://www.boxofficemojo.com/chart/top_lifetime_gross/\"\n",
    "\n",
    "# Sending a request to fetch the content of the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Finding the table\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extracting the table headers\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "# Extracting the rows\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "    cells = []\n",
    "    for cell in row.find_all('td'):\n",
    "        link = cell.find('a')\n",
    "        if link:\n",
    "            # Extracting only the href attribute\n",
    "            cells.append(link.get('href'))\n",
    "        else:\n",
    "            cells.append(cell.text.strip())\n",
    "    if cells:  # Avoid empty rows\n",
    "        rows.append(cells)\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "# NEW STEPS: \n",
    "df['title_id'] = df['Title'].str.split('/').str[2]\n",
    "df['title_page_url'] = 'https://www.boxofficemojo.com/title/' + df['title_id']\n",
    "\n",
    "# Displaying the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really, all we care about is the `title_id` and `title_page_url`, so just return these. Plus wrap it all up into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_title_ids(url):\n",
    "\n",
    "    # Sending a request to fetch the content of the page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Finding the table\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extracting the table headers\n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "    # Extracting the rows\n",
    "    rows = []\n",
    "    for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "        cells = []\n",
    "        for cell in row.find_all('td'):\n",
    "            link = cell.find('a')\n",
    "            if link:\n",
    "                # Extracting only the href attribute\n",
    "                cells.append(link.get('href'))\n",
    "            else:\n",
    "                cells.append(cell.text.strip())\n",
    "        if cells:  # Avoid empty rows\n",
    "            rows.append(cells)\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    # Extract title information...\n",
    "    df['title_id'] = df['Title'].str.split('/').str[2]\n",
    "    df['title_page_url'] = 'https://www.boxofficemojo.com/title/' + df['title_id']\n",
    "    \n",
    "    # ... and drop everything else:\n",
    "    df = df[['title_id', 'title_page_url']]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it on the entire 1000 movie list, spread over 5 pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_title_identifiers_df = pd.concat([\n",
    "    scrape_title_ids(\"https://www.boxofficemojo.com/chart/top_lifetime_gross/?offset=\" + str(offset)) \n",
    "    for offset in [0, 200, 400, 600, 800]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_title_identifiers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1000 title IDs now, in the same order as the original data frame.\n",
    "\n",
    "So what? Well, we can use identifiers such as these to extract more information, including linking to other pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we already have code to scrape the first table that appears on a webpage. Let's run it on the first link in our table (https://www.boxofficemojo.com/title/tt4154796):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_boxoffice_mojo_table_text('https://www.boxofficemojo.com/title/tt4154796')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it pulls out some useful information. But there's other information on different parts of that page.\n",
    "\n",
    "Let's focus on the \"MPAA\", which in this case is \"PG-13\". It's not technically part of a table (although it looks like that to our eyes). Let's ask ChatGPT for help:\n",
    "\n",
    "> Extract the value associated with the \"MPAA\" on the website https://www.boxofficemojo.com/title/tt4154796/\n",
    "\n",
    "This is the code ChatGPT suggested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page\n",
    "url = \"https://www.boxofficemojo.com/title/tt4154796/\"\n",
    "\n",
    "# Sending a request to fetch the content of the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Finding the MPAA rating\n",
    "mpaa_label = soup.find(text=\"MPAA\")\n",
    "if mpaa_label:\n",
    "    mpaa_value = mpaa_label.find_next('span').text.strip()\n",
    "    print(f\"MPAA Rating: {mpaa_value}\")\n",
    "else:\n",
    "    print(\"MPAA Rating not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's very useful! Let's simplify a little, and update the function call based on the warning above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.boxofficemojo.com/title/tt4154796/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "mpaa_label = soup.find(string=\"MPAA\")\n",
    "if mpaa_label:\n",
    "    mpaa_value = mpaa_label.find_next('span').text.strip()\n",
    "else:\n",
    "    mpaa_value = None\n",
    "    \n",
    "mpaa_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap this up into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mpaa_rating(title_page_url):\n",
    "    response = requests.get(title_page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    mpaa_label = soup.find(string=\"MPAA\")\n",
    "    if mpaa_label:\n",
    "        mpaa_value = mpaa_label.find_next('span').text.strip()\n",
    "    else:\n",
    "        mpaa_value = None\n",
    "    return mpaa_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mpaa_rating(\"https://www.boxofficemojo.com/title/tt4154796/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_title_identifiers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_title_identifiers_df['title_page_url'].values[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_mpaa_ratings = [ get_mpaa_rating(title_page_url) for title_page_url in domestic_title_identifiers_df['title_page_url'].values[0:20] ]\n",
    "top20_mpaa_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_domestic_lifetime_gross_df = domestic_lifetime_gross_df.head(20)\n",
    "top20_domestic_lifetime_gross_df['mpaa_rating'] = top20_mpaa_ratings\n",
    "top20_domestic_lifetime_gross_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz, for later\n",
    "\n",
    "* Repeat what I have done for another characteristic of each movie, such as the `Budget` characteristic.\n",
    "* Modify your code (and mine) to extract the MPAA rating and the Budget at the same time, i.e. from the same `soup` variable. This avoids having to download and parse each title page once per attribute. It's best to return them as a dataframe.\n",
    "* If you have time, run this for every one of the 1000 movies in the original `domestic_lifetime_gross_df` dataframe. \n",
    "* Combine all these 1000 results into a single dataframe.\n",
    "* Finally, combine your 1000-row dataframe with the original 1000-row `domestic_lifetime_gross_df` dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
